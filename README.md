# **Linear Regression From Scratch (NumPy Implementation)**

This project implements **linear regression from scratch** using only **NumPy**, without relying on machine learning libraries like scikit-learn.
The goal is to understand the fundamentals of:

* Data generation
* Matrix shapes and preprocessing
* Prediction function
* Mean Squared Error (MSE) loss
* Gradient computation
* Gradient Descent optimization
* Parameter convergence
* Visualization of the learning process

This project demonstrates how linear models learn under the hood.

---

## ğŸ“Š **Project Overview**

We generate synthetic data from the true relationship:

```
y = 3x + 7 + noise
```

Then we implement:

### âœ” Prediction function

Uses matrix multiplication (`X @ w + b`)

### âœ” MSE Loss

```math
MSE = (1/n) * Î£ (y_pred_i - y_i)^2
```

### âœ” Gradients

Computed manually using the derivative of the MSE loss.

### âœ” Gradient Descent

Parameters updated using:

```
w := w - learning_rate * grad_w  
b := b - learning_rate * grad_b
```

### âœ” Training Loop

Runs multiple gradient descent steps until convergence.

---

## ğŸ“‰ **Training Results**

The model successfully learns values close to the true parameters:

* **True w:** 3
* **True b:** 7
* **Learned w:** ~ 3.03
* **Learned b:** ~ 6.61
* **Final loss:** ~ 3.26

The notebook also includes:


### ğŸ“‰ Loss Curve
<img src="images/loss_curve.png" width="500"/>

### ğŸ“ˆ Final Model Fit
<img src="images/fitted_line.png" width="500"/>
  

---

## ğŸ“ **Repository Structure**

```
linear-regression-from-scratch/
â”‚
â”œâ”€â”€ linear_regression_from_scratch.ipynb   # Full implementation
â””â”€â”€ README.md                              # Project description
```

---

## ğŸš€ **Why This Project Matters**

This project demonstrates understanding of key ML concepts:

* Vectorised NumPy computation
* Implementing models manually
* Understanding gradients & optimization
* Clear, maintainable code
* Using visualisations to evaluate learning

This forms a strong foundation for:

* Logistic Regression
* Neural Networks
* Deep Learning
* Regularisation (Ridge/Lasso)
* Advanced optimisers (Adam, Momentum)

---

## ğŸ§  **What I Learned**

* How linear regression works internally
* How gradients drive learning
* How to implement MSE and gradient descent
* How NumPy shapes and broadcasting work
* How to visualise and debug ML code

---

## ğŸ“¦ **Future Improvements**

* Polynomial regression
* Multi-feature regression
* Ridge/Lasso regularisation
* Comparison with scikit-learn
* Stochastic Gradient Descent

---

## âœ¨ **Author**

**Shakur Ahmad**
Aspiring Machine Learning Engineer
GitHub: [shakurahmadd](https://github.com/shakurahmadd)

---

Copy/paste this directly â€” GitHub will render it perfectly.
